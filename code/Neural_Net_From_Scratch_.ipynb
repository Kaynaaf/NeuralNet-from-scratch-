{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a NeuralNet from scratch\n",
        "\n",
        "  \n",
        "\n",
        "### The structure of the project\n",
        "\n",
        "The first order of business was defining a `Layer` class.\n",
        "\n",
        "The `Dense` and `ActivationLayer` Classes would then inherit from that class.\n",
        "\n",
        "Then afterwards, I implemented a `Network` class which will handle the training of the NeuralNet.\n",
        "\n",
        "\n",
        "For the optimiser, I implemented `ADAM` which takes in the gradients obtained by `Network.train()` and then updates the parameters according to their first and second moments.\n",
        "\n",
        "### Results and interpretations\n",
        "\n",
        "I have trained two models, one on a regression task and the other on a  binary classification one to test them out.\n",
        "\n",
        "* For the classification task, I chose the Wisconsin breast cancer dataset.\n",
        "\n",
        "  The model then, needs to predict whether the tumor was benign or not.\n",
        "\n",
        "\n",
        "  The model returned an accuracy of **96.9%** (Train) & **97.7%** (Test).\n",
        "\n",
        "\n",
        "\n",
        "* For the regression task, I chose the California housing dataset.\n",
        "  \n",
        "\n",
        "  Based on the features found in the dataset, the model would then predict the housing prices.\n",
        "\n",
        "  The model returned a *mean squared error* of **0.4071** meaning that, on average, the model's predictions were off by about \\$4,071 (since the prices were in **$100,000**)."
      ],
      "metadata": {
        "id": "vy-HBjG-SuII"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60d0d5e"
      },
      "source": [
        "## Defining the Layer class\n",
        "\n",
        "*A base class for layers with methods for forward and backward passes.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "E6aLRQUUpu0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d3397ad0"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        # Forward Pass\n",
        "\n",
        "        return input_data\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "        # Backward Pass\n",
        "\n",
        "        return output_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57963b1a"
      },
      "source": [
        "\n",
        "## Implementing the dense layer\n",
        "\n",
        "*A dense (fully connected) layer inheriting from the base Layer class.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0cf2218a"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01  # weights initialised with small random values\n",
        "        self.biases = np.zeros((1, output_size))  # biases initialised as 0\n",
        "        self.input_data = None  # input for backward pass\n",
        "        self.weights_gradient = None # weights gradient\n",
        "        self.biases_gradient = None # biases gradient\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        self.input_data = input_data\n",
        "        return np.dot(input_data, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        # Calculate and store gradients\n",
        "        self.weights_gradient = np.dot(self.input_data.T, output_gradient)\n",
        "        self.biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
        "\n",
        "        return input_gradient\n",
        "\n",
        "    def get_params_and_grads(self):\n",
        "\n",
        "        return [\n",
        "            {'params': self.weights, 'grads': self.weights_gradient},\n",
        "            {'params': self.biases, 'grads': self.biases_gradient}\n",
        "        ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "176254e0"
      },
      "source": [
        "## Implementing activation functions\n",
        "\n",
        "*Activation functions like ReLU, Sigmoid and, Tanh which would be used by the NN*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cbbc27ce"
      },
      "outputs": [],
      "source": [
        "# Base Class for activation functions\n",
        "class ActivationLayer(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        return input_data\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        return output_gradient\n",
        "\n",
        "\n",
        "# Various activation functions which inherit from ActivationLayer class\n",
        "class ReLU(ActivationLayer):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.input_data = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        self.input_data = input_data\n",
        "        return np.maximum(0, input_data)\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        relu_gradient = (self.input_data > 0).astype(float)\n",
        "        return output_gradient * relu_gradient\n",
        "\n",
        "class Sigmoid(ActivationLayer):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.output_data = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        output = 1 / (1 + np.exp(-input_data))\n",
        "        self.output_data = output\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        sigmoid_gradient = self.output_data * (1 - self.output_data)\n",
        "        return output_gradient * sigmoid_gradient\n",
        "\n",
        "class Tanh(ActivationLayer):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.output_data = None\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        output = np.tanh(input_data)\n",
        "        self.output_data = output\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        tanh_gradient = 1 - self.output_data**2\n",
        "        return output_gradient * tanh_gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "447c0bf8"
      },
      "source": [
        "## Implementing the network class\n",
        "\n",
        "\n",
        "*A class which representing the Neural net, allowing it to hold the layers and manage the training process*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "c61abcac"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.layers = []\n",
        "\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        output = input_data\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def backward(self, output_gradient):\n",
        "\n",
        "        input_gradient = output_gradient\n",
        "        for layer in reversed(self.layers):\n",
        "            input_gradient = layer.backward(input_gradient)\n",
        "\n",
        "    def get_trainable_params_and_grads(self):\n",
        "        # Collects trainable parameters and their gradients from all layers\n",
        "        params_and_grads = []\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'get_params_and_grads'):\n",
        "                params_and_grads.extend(layer.get_params_and_grads())\n",
        "        return params_and_grads\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, loss_function, optimizer, epochs, batch_size):\n",
        "\n",
        "\n",
        "        num_samples = x_train.shape[0]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            indices = np.arange(num_samples)\n",
        "            np.random.shuffle(indices)\n",
        "            x_train_shuffled = x_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                x_batch = x_train_shuffled[i:i + batch_size]\n",
        "                y_batch = y_train_shuffled[i:i + batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.forward(x_batch)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = loss_function.loss(y_batch, output)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                output_gradient = loss_function.gradient(y_batch, output)\n",
        "                self.backward(output_gradient)\n",
        "\n",
        "                # Update weights\n",
        "                params_and_grads = self.get_trainable_params_and_grads()\n",
        "                optimizer.update(params_and_grads)\n",
        "\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / (num_samples / batch_size)\n",
        "\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ccfd9f"
      },
      "source": [
        "## Implementing a loss function\n",
        "\n",
        "*A class for a loss function like Mean Squared Error or Cross-Entropy with methods for calculating the loss and its gradient*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "09e39d3f"
      },
      "outputs": [],
      "source": [
        "class LossFunction:\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "        # Calculations would be handled by the subclass\n",
        "        raise NotImplementedError(\"Subclass must implement abstract method\")\n",
        "\n",
        "    def gradient(self, y_true, y_pred):\n",
        "\n",
        "        raise NotImplementedError(\"Subclass must implement abstract method\")\n",
        "\n",
        "class MeanSquaredError(LossFunction):\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "\n",
        "        return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "    def gradient(self, y_true, y_pred):\n",
        "\n",
        "        return -2 * (y_true - y_pred) / y_true.shape[0]\n",
        "\n",
        "class CrossEntropyLoss(LossFunction):\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "        # Avoid division by zero or log of zero\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def gradient(self, y_true, y_pred):\n",
        "        # Avoid division by zero\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return (y_pred - y_true) / (y_pred * (1 - y_pred)) / y_true.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a04eec6"
      },
      "source": [
        "## Implementing optimization\n",
        "\n",
        "*Gradient descent is used to update the model weights*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1df450b8"
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "\n",
        "    def __init__(self, learning_rate):\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        # Use lists to store moments corresponding to parameters\n",
        "        self.m = []  # First moment (mean)\n",
        "        self.v = []  # Second moment (variance)\n",
        "        self.beta1 = 0.9  # Exponential decay rate for the first moment estimates\n",
        "        self.beta2 = 0.999 # Exponential decay rate for the second moment estimates\n",
        "        self.epsilon = 1e-8 # Small constant to prevent division by zero\n",
        "        self.t = 0 # Time step\n",
        "        self._params_initialized = False # Flag to check if moments are initialized\n",
        "\n",
        "\n",
        "    def update(self, params_and_grads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params_and_grads: A list of dictionaries, where each dictionary\n",
        "                              contains 'params' (the parameter array) and\n",
        "                              'grads' (the gradient array) for a layer.\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        if not self._params_initialized:\n",
        "             # Initialize moments for each parameter if not already done\n",
        "            for item in params_and_grads:\n",
        "                params = item['params']\n",
        "                self.m.append(np.zeros_like(params))\n",
        "                self.v.append(np.zeros_like(params))\n",
        "            self._params_initialized = True\n",
        "\n",
        "\n",
        "        for i, item in enumerate(params_and_grads):\n",
        "            params = item['params']\n",
        "            grads = item['grads']\n",
        "\n",
        "            # Update biased first and second moment estimates using list indexing\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads**2)\n",
        "\n",
        "            # Compute bias-corrected first and second moment estimates\n",
        "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
        "\n",
        "            # Update parameters\n",
        "            item['params'] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925a38a5"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "The NN can be now trained to solve classification or regression problems\n",
        "\n",
        "I have trained a NeuralNet on the *Wisconsin breast cancer dataset* which then outputs if the tumor is benign or malignant.\n",
        "\n",
        "\n",
        "This is represented by the output of 1 if the tomor is benign and 0 otherwise.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "I also have trained another instance on the California housing dataset, having the model predict the target variable, the house price.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Some preprocessing before feeding the data into the NN\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Network architecture params\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "hidden_size = 32\n",
        "\n",
        "binary_cancer_net = Network()\n",
        "binary_cancer_net.add_layer(Dense(input_size, hidden_size))\n",
        "binary_cancer_net.add_layer(ReLU())\n",
        "binary_cancer_net.add_layer(Dense(hidden_size, output_size))\n",
        "binary_cancer_net.add_layer(Sigmoid())\n",
        "\n",
        "\n",
        "loss_function = CrossEntropyLoss()\n",
        "optimizer = Optimizer(learning_rate=0.001)\n",
        "\n",
        "\n",
        "# Training params\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "binary_cancer_net.train(X_train, y_train, loss_function, optimizer, epochs, batch_size)\n",
        "\n",
        "# Evaluate accuracy\n",
        "def calculate_binary_accuracy(network, X, y_true_binary):\n",
        "    y_pred = network.forward(X)\n",
        "\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
        "    return accuracy\n",
        "\n",
        "train_accuracy = calculate_binary_accuracy(binary_cancer_net, X_train, y_train)\n",
        "test_accuracy = calculate_binary_accuracy(binary_cancer_net, X_test, y_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SNQ14hkhhwm",
        "outputId": "0843e812-791a-4c62-c92a-56afc2c1520a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7183011349741765\n",
            "Epoch 2/10, Loss: 0.6606205362795894\n",
            "Epoch 3/10, Loss: 0.5549023955318061\n",
            "Epoch 4/10, Loss: 0.42851060751544273\n",
            "Epoch 5/10, Loss: 0.321767458799817\n",
            "Epoch 6/10, Loss: 0.2626480949402606\n",
            "Epoch 7/10, Loss: 0.21142753167379766\n",
            "Epoch 8/10, Loss: 0.18130492224094905\n",
            "Epoch 9/10, Loss: 0.16466441640852192\n",
            "Epoch 10/10, Loss: 0.15349563468104513\n",
            "Train Accuracy: 0.9692307692307692\n",
            "Test Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a141b411",
        "outputId": "3e4507fe-8363-4b35-f01b-ba75ff71db48"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target.reshape(-1, 1)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Some preprocessing before feeding the data into the NN\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Network architecture for regression\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "hidden_size = 64\n",
        "\n",
        "network = Network()\n",
        "network.add_layer(Dense(input_size, hidden_size))\n",
        "network.add_layer(ReLU())\n",
        "network.add_layer(Dense(hidden_size, output_size))\n",
        "\n",
        "\n",
        "loss_function = MeanSquaredError()\n",
        "optimizer = Optimizer(learning_rate=0.001)\n",
        "\n",
        "# Training\n",
        "epochs = 15\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "network.train(X_train, y_train, loss_function, optimizer, epochs, batch_size)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = network.forward(X_test)\n",
        "\n",
        "# Calculate regression metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "print(f\"Mean Absolute Error on Test Set: {mae}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 0.669293501963419\n",
            "Epoch 2/15, Loss: 0.41949338893147536\n",
            "Epoch 3/15, Loss: 0.39797497417166294\n",
            "Epoch 4/15, Loss: 0.38579965617784356\n",
            "Epoch 5/15, Loss: 0.37550733829900773\n",
            "Epoch 6/15, Loss: 0.36599470717713933\n",
            "Epoch 7/15, Loss: 0.36215694366250467\n",
            "Epoch 8/15, Loss: 0.3569228348420627\n",
            "Epoch 9/15, Loss: 0.3568232791435399\n",
            "Epoch 10/15, Loss: 0.3546660956416732\n",
            "Epoch 11/15, Loss: 0.3569506050072357\n",
            "Epoch 12/15, Loss: 0.348592258471895\n",
            "Epoch 13/15, Loss: 0.34649980532332153\n",
            "Epoch 14/15, Loss: 0.34104329423008223\n",
            "Epoch 15/15, Loss: 0.3378038142828495\n",
            "Mean Squared Error on Test Set: 0.3444202377685985\n",
            "Mean Absolute Error on Test Set: 0.40713583657078456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single data point\n",
        "data_point = X_test[0:1] #\n",
        "\n",
        "\n",
        "\n",
        "# Make a prediction using the trained network\n",
        "prediction = network.forward(data_point)\n",
        "\n",
        "predicted_value = prediction[0][0]\n",
        "actual_value = y_test[0][0]\n",
        "\n",
        "print(f\"Predicted housing value: {predicted_value}\")\n",
        "print(f\"Actual housing value: {actual_value}\")\n",
        "\n",
        "# Calculate percentage error\n",
        "if actual_value != 0:\n",
        "    percentage_error = ((actual_value - predicted_value) / actual_value) * 100\n",
        "    print(f\"Percentage Error: {percentage_error:.2f}%\")\n",
        "else:\n",
        "    print(\"Cannot calculate percentage error as the actual value is zero.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6odor6-1ey7D",
        "outputId": "e140907c-d51f-4e3a-eb14-9dea98b2255d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted housing value: 0.6358440305750879\n",
            "Actual housing value: 0.477\n",
            "Percentage Error: -33.30%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}